\chapter{From Noisy Spots to Accurate Clonal Maps: A Bayesian Approach}
\label{sec:chapter-basiss-model}

\section*{Contributions}
This chapter is largely based on the supplementary methods and technical results from:
\\~\\
 \fullauthcite{Lomakin2022-ks}. 
\\~\\
The work I present here is primarily my own contribution. I focused on developing and implementing the core mathematical model for the \acs{BaSISS} data under the supervision of \ac{moritz}, with valuable inputs from \ac{artem1} and \ac{vitalii}. I analysed and interpreted the data, and drafted the original article and figures, again under the supervision of \ac{moritz} and \ac{lucy}. You can find all the code on \href{https://github.com/gerstung-lab/BaSISS}{Github}.

Another first author of this paper, \ac{jessica}, in collaboration with \ac{peter}, \ac{mats}, and \ac{lucy}, designed the initial study of \acs{BaSISS}. \ac{jessica}, \ac{mats}, and \ac{carina} conducted the experiments and provided the raw \acs{BaSISS} data. \ac{junsung}, \ac{vasyl}, \ac{tong}, and \ac{milana} preprocessed and decoded this data. \ac{lucy} performed the \acs{LCM} cuts, and \ac{stefan} conducted the \acs{WGS} subclonality analyses. I had access to the decoded \acs{BaSISS} and \acs{LCM}-\acs{WGS} data.

The Introductory \cref{sec:basiss-popular} and \cref{sec:bayesian-intro}, Data Characterisation \cref{sec:bassis-data-brief}, and Validation \cref{sec:bassis-validation} are original. I reworked the rest of the sections to fit the thesis format, rewording, expanding and illustrating them for clarity. All the main and margin figures are original except \cref{fig:basiss-data-raw}, \cref{fig:basiss-replication-stats}, \cref{fig:conceptual-bassis-model} and \cref{fig:basiss-lcm-validation}, which I borrowed from the original paper with stylistic and compositional adjustements. 

\section{Background}
\subsection{The \ac{BaSISS} protocol}
\label{sec:basiss-intro}
Lineage tracing through somatic mutations can reveal ancestral connections between cancer subclones \pcref{sec:chapter-introduction}. Yet, current methods fall short in preserved human tissue studies \parencite{Yates2015-xk,Jamal-Hanjani2017-uv,Jones2008-sd,Shah2009-xz,Casasent2018-gx,Tarabichi2021-xx}. Techniques like \acf{LCM} combined with targeted nucleic acid library sequencing, including single-cell sequencing, help to some extent in delineating subclone spatial structures \parencite{Shen2000-xj,Casasent2018-gx}. However, \ac{LCM}-based methods fail to capture the full diversity of cancer clones across entire tumour sections. Even the most exhaustive sampling approach would struggle to profile square centimetres at the desired cellular or slightly above cellular resolution. While recent spatial genomics techniques can identify cancer clones based on distinct copy number profiles \parencite{Zhao2022-xd,Erickson2022-zh}, they fall short in detecting point mutations or quantifying mixed clones.

Though in situ hybridization \parencite{Janiszewska2015-kb} and mutation-specific padlock probes \parencite{Larsson2010-bp,Grundberg2013-te,Ke2013-ux,Baker2017-dv} can detect individual mutations in situ, their potential is limited by the number of available fluorophores, allowing them to trace only up to 4 mutations at once. Considering the genetic uniqueness of each cancer and its subclones, tracing multiple specific somatic mutations is crucial to decipher spatial and temporal ancestral relationships \parencite{Nik-Zainal2012-zz}. The solution lies in encoding in situ sequencing probes combinatorially, allowing them to be captured with a multi-round imaging strategy, thus enabling multiplex \acf{BaSISS}.

The \explain{\ac{BaSISS} protocol}{has two essential stages: 1. Bulk \ac{WGS} identification of clones and mutations, 2. multiplex \acs{ISS} imaging \marginfig{side-plot-basiss-short.pdf}} uses bulk \ac{WGS} and z-stacked sections for spatial clone mapping (The protocol is introduced in detail in section \cref{sec:basiss-full-protocol}). After identifying subclones from bulk WGS, \ac{BaSISS} employs two main steps. First, it uses padlock probes designed for both mutant and wild-type alleles, marked with a unique 4–5 nucleotide barcode for multiplexing \parencite{Ke2013-ux}. These probes can target any expressed somatic mutation, including rearrangement breakpoints, and can be supplemented with copy number alterations. Next, \ac{BaSISS}  uses a cyclical microscopy approach, akin to gene expression in situ sequencing protocol \parencite{Ke2013-ux,Svedlund2019-xb} \pcref{box:technologies}.

The procedures described, along with initial data processing, produce two primary sets of information. \ac{WGS} reveals anticipated clonal expansions, identified by point mutations and copy number variations specific to each clone. The spatial component locates alleles identified by \ac{WGS} in the tissue section. We also obtain a proxy to determine cell locations through segmented nuclei locations. Nevertheless, the interpretation of subclone locations becomes complicated due to the inherent complexity of the cancer genome, tissue heterogeneity, and potential systematic errors in the technology. The latter poses significant challenges due to the relatively low sensitivity of the \ac{BaSISS} data (< 1 singal per cell) and off-target hybridisation of the padlock probes (\cref{sec:bassis-data-brief} and \cref{fig:basiss-data-raw}, aslo discussed in \cref{sec:allelic-confusion}). These challenges drive the need for a robust statistical approach.

\subsection{Popular approaches in modelling spatial transcriptomics data}
\label{sec:basiss-popular}
Spatial inference is a common challenge spanning various research fields such as economics, geology, and ecology. Consequently, the field of spatial statistics has become rich in tools and methodologies, often finding applications well beyond their original purpose. For instance, \acl{GP} invented solely for geostatistics \parencite{Krige1951-mt}, now find use in diverse areas such as optimization problems, finance, and astronomy. This cross-applicability of tools is indicative of the universal nature of spatial problems. Even in spatial genomics, a relatively recent field, there exists a wide array of mathematical tools that can be applied to solve its unique challenges.
    
While the overarching framework of spatial statistics offers many general tools, each field inevitably cultivates its own preferred methodologies. These preferences often arise from the specific nature of the data and the underlying scientific understanding. In spatial genomics, the uniqueness stems from both the type of data generated and the biological insight that guides the analysis. Often, the task narrows down to mapping and characterising cells, the structural and functional units of the body, using precise markers and observed spatial distributions. Researchers frequently perform these tasks using \explain{latent variable model}{presented as probabilistic graphical model \marginfig{side-plot-lvm.pdf} Latent (hidden) variables $M$ and $G$ control behaviour of the ovbserved variable $D$}ling. Here, observed spatial distributions $D_{x,y,a}$ of features $a$ (e.g. expression) in spatial dimensions $x, y$ relate to the spatial distribution $M_{x,y,c}$ of $c$ common factors $G_{c,a}$ (e.g. cell types or expression programmes), as illustrated by:

\begin{equation}
    D_{x,y,a} = f(M_{x,y,c}, G_{c,a})
\end{equation}

The mapping f may be a complex neural network \parencite{Dong2022-ty, Ma2022-pd} or as simple as a linear operator:

\begin{equation}
    D_{x,y,a} = \sum_{c} M_{x,y,c} \cdot G_{c,a}
\end{equation}

However, even in the simplest case of a linear mapping, an essential consideration emerges: if the latent variables remain unconstrained, there is no assurance that the latent representation in $G_{.,a} \in \mathbb{R} ^c$ will mirror the actual cell types. Generally, this decomposition is not uniquely identifiable, leading to significant issues for interpretation and numerical estimations.

This necessitates the imposition of constraints, whether applied individually or collectively. Constraints may apply to the spatial component $M$, utilising tools such as \acf{GP} \parencite{Townes2023-uj}, Markov Random Fields \parencite{Petukhov2022-pv}, and Conditional Autoregression \parencite{Ma2022-pd, Ni2022-tu}.

Alternatively, constraining the latent variable $G$ is achievable by leveraging the complementary aspects of single cell or bulk genomic data. This includes learning a combined latent space representation from both single cell and spatial data or extracting ``cell type" signatures to guide the spatial model. The latter approach, often referred to as \emph{cell-type deconvolution}\footnote{In my opinion, a more apropriate name is the alternative term \emph{сell-type decomposition}. While I can see the metaphorical value, it is confusing that the term deconvolution is used on the data where no explicit convolution operations were modelled}, correlates closely with the conjecture for \ac{BaSISS} data. This requires mapping of clones $c$ - cell types defined by specific combinations of known allelic variants $a$ - in tissue space based on these alleles' spatial distribution.

Recent benchmarking within the wide array of spatial transcriptomics models has shown that generative Bayesian models excel in cell type decomposition \parencite{Li2023-ik}. Considering additional favourable properties such as interpretability and direct uncertainty estimation, this class of models often becomes an appealing choice for modelling tasks within the field.

\subsection{Bayesian statistical modelling}
\label{sec:bayesian-intro}

In many practical contexts, we find ourselves in situations where the underlying mechanisms governing the system are hidden or only partially observed, and the observations are noisy and limited. Such scarcity of information introduces inherent uncertainty in our understanding of the system. It is within this landscape of uncertainty that the Bayesian understanding of probabilities emerges as a natural tool. Rather than mere frequencies, probabilities are construed as representations of the degree of belief, capturing the epistemic uncertainty about the system parameters. A statistical model, crafted within this Bayesian paradigm, acts as a bridge between the observed data and the underlying phenomena, translating our assumptions and knowledge about the problem into a coherent probabilistic framework. This in turn allows us to make predictions, perform inference, and understand more deeply the structure of the data.

\subsubsection*{Basics of Bayesian inference}
Given a set of observations ${D} = \{t_1, t_2, … t_n\}$, a statistical model aims to describe how these observations are generated. One can consider these observations a realisation of a random variable $T$ which is controlled by a set of parameters $\theta$. Therefore, a statistical model could formally be defined by specifying the joint probability distribution $p({D}, \theta)$, describing the stochastic relationship between the observations and the controlling parameters. We can rewrite the joined probability using conditional probability, leading us to the Bayes' theorem equation:

\begin{equation}
p(\theta | {D}) = \frac{p({D} | \theta) p(\theta)}{p({D})} = \frac{p({D} | \theta) p(\theta)}{\int p({D} | \theta) p(\theta) d\theta}
\end{equation}

Here $p({D}|\theta)$ is the likelihood, representing the probability of the observations given the parameters; $p(\theta)$ is the prior distribution over the parameters, reflecting our beliefs about $\theta$ before observing the data; $({D}| \theta)$ is the posterior distribution after observing the data; and $p({D})$ is the evidence, obtained by marginalising $\theta$ out of the joined distribution. It is essentially a normalising constant, ensuring that posterior is a valid probability distribution and integrates to one. 
Both frequentist and Bayesian models rely on the likelihood $p({D}|\theta)$, but they interpret the parameters $\theta$ differently. In the frequentist perspective, $\theta$ is fixed and its confidence intervals are estimated by considering multiple repeated samples of ${D}$. In the Bayesian view, $\theta$ varies for each observation set of ${D}$ and uncertainty in its estimation is expressed as a probability distribution. With this understanding, we can view statistical inference as an update on our prior belief of the distribution of hidden parameters based on additional evidence.

\begin{equation}
\text{posterior} \propto \text{likelihood} \times \text{prior}
\end{equation}

To demonstrate, let’s consider a simple gamma-poisson model of the gene expression, with the gene expression rate as a hidden variable. Consider an observation $D_n$  of observed counts in $n$ cells. I will model the gene expression count for each cell $i$ as a $\text{Poisson}$ random variable with the rate parameter $\lambda$ which has a $\text{Gamma}$ prior, where $\alpha$ and $\beta$ are the hyperparameters shaping our initial belief about expression rate.

\begin{align}
    D_i & \sim \text{Poisson}(\lambda) = \frac{\lambda^{D_i} e^{-\lambda}}{D_i!} \\
    \lambda & \sim \text{Gamma}(\alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha - 1} e^{-\beta \lambda}
\end{align}
    
Using the fact that the likelihood is independent Poisson distributions and the prior is a Gamma distribution it is trivial to derive the analytical form of the posterior $p(\lambda|D)$:

\begin{align}
        p(\lambda|D) & \propto p(D|\lambda) p(\lambda) \\
         & \propto \prod_{i=1}^N \frac{\lambda^{D_i} e^{-\lambda}}{D_i!} \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha - 1} e^{-\beta \lambda} \\ 
        & \propto  \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\left(\sum_{i=1}^N D_i\right) + \alpha - 1} e^{-\lambda(n+\beta)} \\
         & = \text{Gamma}(\alpha + \sum_i^n D_i, \beta + n)
        \label{eq:simple-gamma-poisson}
\end{align}

The graphical depiction of the components is shown on \cref{fig:simple-gamma-poisson}.

\figuretextwidth{simple-gamma-poisson.pdf}{simple-gamma-poisson}
    {Simple gamma-poisson model of cell expression.}
    {Toy mathematical model of gene expression in cells trained on $n$ = 5 observations. The gene expression rate $\lambda$ is a hidden variable, which is modelled as a $\text{Gamma}$ random variable with the hyperparameters $\alpha$ and $\beta$ which is consistent with the week prior beliefe on low expression. The observed counts $D_i$ are modelled as $\text{Poisson}$ random variables with the rate parameter $\lambda$. An exact analytical solution for the posterior distribution $p(\lambda|D)$ is tracktable (\cref{eq:simple-gamma-poisson}) due to the conjugacy of the prior and likelihood.}

\subsubsection*{Bayesian inference in practice}
The process of inference, while appearing simple in principle, presents substantial difficulties in actual computation. A major obstacle lies in the denominator of the Bayesian formulation, the integral often becomes intractable  thus rendering even moderately complex Bayesian models intractable, apart from a few cases with carefully selected \explain{ \emph{conjugate priors}}{are prior distributions that, when combined with a specific likelihood function, result in a posterior distribution with the same functional form} like shown in the previous example. This issue is a significant constraint in the applicability of Bayesian models, which must be carefully considered in their practical implementation and model formulation.

The solution to the problems came through a sequence of developments in \ac{MCMC} methods, including the Metropolis algorithm \parencite{Rosenbluth1953-gu}, Metropolis-Hastings algorithm \parencite{Hastings1970-mb}, Gibbs sampling \parencite{Geman1984-nk}, and Hybrid Monte Carlo (HMC) \parencite{Duane1987-gu}. These methods permit the generation of samples from the posterior probability distribution by forming an invertible Markov chain. The equilibrium distribution of this chain corresponds to the targeted posterior distribution \parencite{Roberts2004-op}. Consequently, if one were to compute the constructed Markov chain, a guarantee exists for the convergence to the genuine posterior distribution of the model, given that the number of sampling steps approaches infinity. In practical terms, provided the model is appropriately specified, convergence tends to occur in a sensible duration. The samples derived from the converged chain may then be utilised to estimate the requisite summary statistics. 

\ac{MCMC} has revolutionised the field of Bayesian inference, segregating the procedures of model formulation and inference. It must be noted, however, that these methods do not scale with ease when faced with the dimensions and complexity of the model. They often either demand an extensive number of steps or involve calculations that, although fewer in number, are considerably computationally demanding. In scenarios where the analytical posterior is inaccessible, and the computational cost of \ac{MCMC} is substantial, it is common to use alternative approximation schemes such as \ac{VI}.

\subsubsection*{Variational inference}
\acl{VI} is framed as an optimization problem, where the objective is to identify a distribution within a constrained family of tractable distributions that most closely approximates the target distribution \parencite{Blei2017-dm}. This technique can be seen as an optimization problem, where the goal is to find a distribution that is most similar to the target distribution, within a constrained family of tractable distributions. Mathematically, the problem of \acl{VI} can be formulated as finding the distribution $q(\theta)$ of the variational family $\mathcal{Q}$ that is following the true posterior $p(\theta | D)$ as closely as possible. The common distance measure that is used is the Kullback-Leibler (KL) divergence, thus the goal is:

\begin{equation}
q(\theta) = \argmin_{q \in \mathcal{Q}} \text{KL}(q(\theta) || p(\theta | D))  
\end{equation}

After expanding KL divergence and rewriting conditional probability:

\begin{align}
q(\theta) &= \argmin_{q \in \mathcal{Q}} \int q(\theta) \log \frac{q(\theta) p(D)}{p(\theta, D)} d\theta \\
&= \argmin_{q \in \mathcal{Q}} \int q(\theta) [\log q(\theta) + \log p(D) - \log p(\theta, D)] d\theta \\
&= \argmin_{q \in \mathcal{Q}} \mathbb{E}_q [\log q(\theta) -  \log p(\theta, D)] + \log p(D)
\end{align}

Notice that $\log p(x)$ is constant with respect to $q$, so we can ignore it when minimising KL divergence. Now we can define the optimisation objective, which is the inverse expectation term, referred as the \explain{Evidence Lower Bound}{\marginfig{side-plot-elbo.pdf}}. Hence, the original problem of minimising KL divergence between approximation distribution and posterior becomes equivalent to the maximisation of \ac{ELBO}. 

One significant development in \ac{VI} is \ac{ADVI} \parencite{Kucukelbir2016-gn}. \ac{ADVI} generalises the approach by employing the normal family of distributions as the guide, thereby enabling the application of gradient-based optimisation techniques, via \explain{reparameterization trick}{allows for the sampling of a variable \( Z \sim \mathcal{N}(\mu, \sigma^2) \) to be reformulated as \( Z = \mu + \sigma \cdot \epsilon \), where \( \epsilon \sim \mathcal{N}(0, 1) \). This transformation makes it possible to compute gradients with respect to \( \mu \) and \( \sigma \).}.

In \ac{ADVI}, the variational distribution is assumed to be multivariate normal with an appropriate transformation.
\begin{equation}
q(\theta) = f(\mathcal{N}(\mu, \Sigma))
\end{equation}

Here, $\mu$ and $\sigma$ are the optimised mean and covariance parameters, and $f$ is a transformation function that maps the normal distribution to the desired distribution. The transformation function is chosen such that the support of the transformed distribution is the same as the support of the target distribution. For example, if the target distribution is a Gamma distribution, then the transformation function is the Exponential function. 

In practice, additional constraints are imposed on the covariance matrix, considering it either low-rank or simply diagonal (mean-field) \parencite{Blei2017-dm}. Although these assumptions may overlook a probable covariance structure between model parameters, and frequently underestimate the variance, they dramatically decrease the number of parameters that can be optimised. This makes the inference of even larger models feasible.

\subsubsection*{Probabilistic programming}
In addition to advancements in inference algorithms, contemporary machine learning frameworks like \ac{Theano}\footnote{A discontinued python framework that I had a misfortune to use as a backand for model optimisation defined in PyMC package. Currently a fork is maintained under the name Aesara}, \ac{PyTorch}, \ac{TensorFlow}, and \ac{JAX}, have notably improved the efficiency of Bayesian inference. This is particularly true for methods that utilise automatic differentiation, such as the Hamiltonian Monte Carlo \ac{MCMC} family techniques and \ac{ADVI}. Probabilistic frameworks, including \ac{PyMC}, \ac{Pyro}, and \ac{NumPyro}, are simplifying the Bayesian model development process. By managing most of the underlying calculations, these frameworks allow researchers to concentrate more on conceptual model development and spend less time on numerical details.

\ac{BaSISS} profiles extensive areas of tissue, encompassing hundreds of thousands of cells. The underlying processes that generate the final data are complex. Therefore, defining a statistical approach becomes unfeasible without the utilisation of cutting-edge inference approaches and frameworks capable of handling this complexity.

\section{A bayesian model to localise cancer clones in tissue}
\subsection{BaSISS data characterisation}
\label{sec:bassis-data-brief}

\figuretextwidth{basiss-data-raw.pdf}{basiss-data-raw}
    {Raw \ac{BaSISS} data for three samples from P1 (P1-ER1, P1-ER2, and P1-D1)}
    {Selected targets are coloured by the clonal origin of the mutated allelic variant. \ac{WGS} estimated \acl{CCF} and clonal phylogeny are shown on the top. The bottom pannel presents the link between clone genotypes and \ac{BaSISS} signal density. Scale bar = 2.5 mm}

Before diving into the model design, let's first examine the raw \ac{BaSISS} data for three samples from P1 (P1-ER1, P1-ER2, and P1-D1). A comprehensive description of the samples and the experimental procedure is discussed in \cref{sec:basiss_data}. In brief, the multi-region \ac{WGS} identified six clonal expansions, and 51 padlock probes were designed to track them on tissue slides targeting mutant and wildtype allelic variants. In total, these three samples contain approximately 1,000,000 cells, spanning 245 mm\(^2\) of tissue (Table \cref{tab:basiss-data-raw}), and roughly 1,000,000 detected \ac{BaSISS} signals, 25\% of which target mutant variants.

If one is to colour allelic variants by their clonal origin, it is easy to note that the data has hierarchical structure \cref{fig:basiss-data-raw}. For example private mutations of \green{green} and \blue{blue} clones tend to colocalise, while avoiding \orange{orange} clone private mutations. This is consistent with the clonal phylogeny, where \green{green} is a descendent of \blue{blue}, and therefore possesses all \blue{blue} mutations, while \orange{orange} clone is a separate lineage. When it comes to the corresponding wild type allels the situation is reversed, in the suspected location of \green{green} clone, wild type variants of the genes where it has a mutation is depleted, which makes sense as the copy number of the wild type variant is decreased once mutation happens. An even more obvious relation between copy number and singal density is observed for the copy number variable locus of \gene{FGFR1}. Since \orange{orange} clone possesses a greater copy number gain of 12 in this locus, the signal density is increased in the corresponding region \cref{fig:basiss-data-raw}. 

These observations motivate two key model design decisions. First of all, it is obvious that we won't be able to extract single cell level information from the data, as the signal density is too low. Therefore, we will be working with aggregated data, where the unit of observation is a region, not a cell. Cells inside these regions can belong to different clonal lineages and each should contribute to the observed signal count. Secondly, there is a clear relation between the copy number and signal density, which suggests that we should incorporate copy number information into the model.

\subsection{Non-negative matrix factorisation}
As discussed previously, the \ac{BaSISS} protocol involves recording a series of fluorescent spots, each of which corresponds to a targeted allele. These spots are decoded into a class of different barcodes $A$, resulting in a table of tuples $(a_i, x_i, y_i)$, where $a_i$ represents the allele of spot $i$, and $x_i$ and $y_i$ denote its respective two-dimensional coordinates on a grid.

This grid can be visualised as a three-dimensional array $\mathbf{D} \in \mathbb{N}^{|a| \times |x| \times |y|}$, where $a$ refers to the specific allele, and $x$ and $y$ correspond to coarse-grained coordinates on a regular grid of dimensions $|x| \times |y|$. The grid size was selected to be 108.8 $\mu$ m, a choice that takes into consideration a balance between data sparsity, precision, and computational cost.

The essential concept behind this approach is that the expected number of \ac{BaSISS} signals can be decomposed into maps of $|s|$ distinct clones $s$, each represented as $\mathbf{M} \in \mathbb{R}^{|s| \times |x| \times |y|}$, and a genotype $\mathbf{G} \in \mathbb{N}^{|a| \times |s|}$,

\begin{equation}
    \mathbb{E}[\mathbf{D}] \approx \mathbf{G} \times \mathbf{M} = \sum_{s} \mathbf{G}_{a, s} \mathbf{M}_{s, x, y}
    \label{eq:ED}
\end{equation}

The genotype matrix $\mathbf{G}$ encapsulates the information about the number of each allelic copy present in each clone $s$. Essentially, $\mathbf{G}$ serves as a matrix representation of the underlying phylogenetic tree, providing a detailed guide to the allelic configuration at each branch of the tree. 

The maps $\mathbf{M}$, on the other hand, offer insight into the relative prevalence of each clone in a specific area of the grid. The maps are modelled as non-negative matrices, as the number of cells in a given region cannot be negative. The non-negativity constraint is also biologically relevant, as it is impossible for a clone to have a negative prevalence in a given region.

Since all three matrices are non-negative, the problem can be categorised as a \emph{non-negative matrix factorisation} problem. It is a known characteristic of such problems that they typically do not have unique solutions. Therefore, additional constraints, guided by biological reasoning, are required to ensure the identifiability of the solution. While the matrix $\mathbf{G}$ possesses a clear biological interpretation and is essentially fixed, the spatual nature of matrix $\mathbf{M}$, being a latent variable, must be carefully defined.

\subsection{Latent Gaussian Process}

Since the matrix $\mathbf{M}$ is designed to represent a spatially continuous map, we can utilise the inherent assumptions about spatial locality present within the data. Specifically, it is logically consistent to anticipate that points that are spatially closer to one another should display higher similarity than those further apart. This expectation not only arises from a general consideration of spatial smoothness but also finds roots in biological rationale. Considering the process of \explain{cancer growth}{can be effectively modelled using the RBF kernel, as the cohesion in many tumours causes descendant cells to stay close together, mirroring the localised similarity patterns that this kernel captures \marginfig{side-plot-cancer-growth-cartoon.pdf}}, for example, dividing cells are typically situated near their parent cells, which further accentuates local similarity. This concept of spatial locality can be mathematically articulated using a Radial Basis kernel function:

\begin{equation}
    K_x(x,x') = \exp\left[ -\dfrac{(x-x')^2}{2l^2}\right]
\end{equation}

Here, $x$ and $x'$ denote spatial coordinates, and $l$ is a length scale hyperparameter that governs the smoothness.

\figuretextwidth{gp-prior-samples.pdf}{gp-prior-samples}
    {Samples from a softmax transformed \ac{GP} under different $\tau$}
    {Samples from a softmax transformed \ac{GP} under different $\tau$. The \ac{GP} is defined by a squared exponential kernel with length scale $l=0.1$. The samples are transformed using a softmax transformation with different temperatures $\tau$. The temperature hyperparameter controls the level of intermixing between the clones}


We can naturally express a generative process constrained by the kernel function through the \acf{GP}. The \ac{GP} represents a stochastic process where any finite set of random variables exhibits a joint Gaussian distribution. Put simply, a \ac{GP} serves as a distribution over functions which shape is constrained by the kernel. 

The \ac{GP} relies on two main components for its specification: a mean function $m(x)$ and a covariance function. Since we are dealing with a two-dimensional spatial map, where $x$ and $y$ dimensions are independent, the covariance function is defined as the product of two one-dimensional kernels, $K_{xy} = K_x \times K_y$. As there is no prior knowledge about the expected prevalence of clones in the region, the mean function is set to zero:

\begin{equation}
    m_s(x,y) \sim \mathcal{GP}(0, K_x \otimes K_y)
\end{equation}

The \ac{GP} is a distribution over functions that belong to the $\mathbb{R}$ domain. However, 
we want $\mathbf{M}$ to be non-negative, and even more specifically, to be a probability simplex. To achieve this, we apply a \emph{softmax transformation} to the \ac{GP} output, which maps the real-valued function to a probability simplex. The softmax transformation is defined as follows:

\begin{equation}
    \mathbf{M} = \left[ \dfrac{e^{\tau m_0}}{1 + \sum_{i=0}^{|s|-1}{e^{\tau m_i}}}, \dots, \dfrac{e^{\tau m_{|s|-1}}}{1 + \sum_{i=0}^{|s|-1}{e^{\tau m_i}}} \right]
\end{equation}

Here, $\tau$ is a temperature hyperparameter that controls the prior expectation of clone intermixing. By setting high $\tau$ values, we express our beliefe that clones are likely to be spatially separated, while higher values of $\tau$ indicate a higher degree of intermixing as shown on \cref{fig:gp-prior-samples}. Since I didn't have a strong expectation about the degree of intermixing, I set $\tau$ = 2 for the case with 6 clones, which corresponds to a weak belief that clones tend to intermix.

The use of \ac{GP} as a prior for the spatial maps $\mathbf{M}$ is a convinient way to deal with the sparcity of the data observed in the \ac{BaSISS} data. It allows to propagate information over the spatial dimensions in a \explain{non-parametric}{Even though \ac{GP} include kernel hyperparameters, the term "non-parametric" here refers to the fact that the \ac{GP} doesn't assume a fixed form for the underlying function it's modeling. The model can adapt to the underlying structure in the data and can represent an infinite variety of functional forms} way, stabilising the solution and reducing the impact of sampling fluctuations. Nevertheless, the model would benefit from excplicit parametrisation of the known sources of variation.

\subsection{Sources of variation}

I model the expected number of \ac{BaSISS} signals, denoted by $\mathbb{E}[\mathbf{D}]$, within a specific region. This calculation involves two components: the genotype matrix $\mathbf{G}$ and the spatial map $\mathbf{M}$. The matrix $\mathbf{G}$ encodes integer values representing the allelic copy number within each cell of a particular clone, while the spatial map $\mathbf{M}$ shows the relative prevalence of each clone in the region.

The product of $\mathbf{G}$ and $\mathbf{M}$ provides a value that one might interpret as the average allelic copy number in the given region. However, this interpretation differs significantly from the expected number of BaSISS signals: the fluarophores observed as a product of sequencing by ligation of the amplified padlock probes hybridised with cDNA reads synthesised from the mRNA expressed by cells in the region. Hence, in order to rescale the product of $\mathbf{G}$ and $\mathbf{M}$ to the expected number of BaSISS signals, we need to account for the following sources of variation:

\begin{enumerate}
    \item Cellular density $\nu$
    \item Differential probe specificity $\iota$
    \item Allelic confusion $\tau$
    \item Clone expression variations $\gamma$
    \item Homogeneous and inhomogeneous background adjustments $\beta$
    \item Overdispersed sampling fluctuations $\alpha$
\end{enumerate}

Accounting for these, the equation for the expected number of BaSISS signals \cref{eq:ED} becomes:

\begin{equation} 
\mathbf{M}_{a, x,y} = \underbrace{\nu_{x,y}}_{\text{cell density}} \cdot \overbrace{\iota}_a^{\mathclap{\text{detection rate}}} \cdot \underbrace{\sum_{a'} \tau_{a,a'}}_{\text{probe confusion}} \sum_s  \overbrace{\gamma_{s,a}}^{\mathclap{\text{clone-specific expression}}} \underbrace{\mathbf{G}_{a,s} \mathbf{M}_{s,x,y}}_{\text{clone contribution}} + \overbrace{\beta_a}^{\mathclap{\text{background}}}
\label{eq:ED_full}\\
\end{equation} 

Let's discuss each of these components in more detail.

\subsubsection*{Cellular density}
The cellular density $\nu_{x,y}$ directly influences the number of observed signals in a given region. While this density represents the expected number of cells, directly observing it poses a challenge. Some tools attempt to infer cellular density from transcriptional signals alone \parencite{Kleshchevnikov2022-ub}. However, this method is unreliable due to \explain{confounding expression}{Same signal level could be explained by higher global expression, or more dense cell populations} specific factors.

Fortunately, the number of nuclei in the region provides necessery information to disentangle the effect of cellular density from the other sources of variation. We obtain this data during the \ac{BaSISS} signal acquisition process. Using a neural network model, we segment the DAPI stained image of nuclei, saving the nuclei counts in a two-dimensional array $\mathbf{N} \in \mathbb{N}^{|x| \times |y|}$. These counts may not always accurately reflect cell densities. This inaccuracy can result from segmentation errors or because the slide captures only a fraction of the cells. As a result, we model the nuclei counts $\mathbb{N}_{x,y}$ as Poisson distributed values. Their mean, $\nu_{x,y}$, represents cell densities and follows a weak Gamma prior.

\begin{equation}
    \mathbf{N}_{x,y} \sim \text{Poisson}({\nu}_{x,y})
\end{equation}
\begin{equation}
    {\nu}_{x,y} \sim \text{Gamma}(\mu, \sigma)
\end{equation}

The prior parameters $\mu$ and $\sigma$ are set to a weekly informative distribution center around the expected number of cells in the region.

\subsubsection*{Differential probe specificity}

\ac{BaSISS} ability to detect allelic variants relies on a multistage process of reverse transcription, padlock probes annealing, amplification and fluorophore detection \parencite{Svedlund2019-xb}. As it is hard to model each of the steps explicitly, we model the overall detection rate $\iota \in \mathbb{R}^{|a|}_{+}$ for each allele $a$ as a Gamma distributed random variable.

\begin{equation}
    {\iota}_{a} \sim \text{Gamma}(\mu, \sigma)
\end{equation}

The prior parameters $\mu$ and $\sigma$ are set to a weekly informative distribution with the probability density shifterd towards zero, as we do not know the average probe specific detection rate, but it is likely to be low given that we observe < 1 \ac{BaSISS} signal per cell.

\subsubsection*{Allelic confusion}
\label{sec:allelic-confusion}

The difference between wild type and mutated allelic variants often amounts to a single nucleotide. This elevates the chances for the padlock probe to anneal to the off-target allelic variant, resulting in \explain{allelic confusion}{\marginfig{side-plot-probe-mismatch.pdf} Even with the mismatch between padlock-probe and target sequence, DNA Ligase is capable of ligating the break}. Though DNA ligases are optimised for fidelity, they can make mistakes, particularly under suboptimal conditions \parencite{Lohman2016-ec}. This mistake-prone behaviour particularly contaminates mutated alleles. Since most cells contain at least two wild type copies, this creates spurious noise, as seen in \cref{fig:basiss-data-raw} (red dots in P1-D1).

To model this effect, I design a sparse transition matrix ${\tau} \in \mathbb{R}^{|a|, |a|}_{(0,1)} $, populating it with transition probabilities $ \{ {\tau}_{a,a'}\} $ for each allele. I impose a strong regularising Beta prior on $ \tau_{a,a'} $, shifting the probability density close to 1 and preventing it from dropping below 0.75. This restriction avoids non-identifiabilities and is a reasonable constraint, given that this effect is selected against during protocol optimisation.

\begin{equation}
 {\tau} = \left(\begin{array}{c|cccc}
       & \ldots & \text{wt} & \text{mut} & \ldots \\ 
\hline
 \vdots &  &        &        & \\
\text{wt}      &  & \tau_{11} & 1-\tau_{11} &  \\
\text{mut}     &  & 1-\tau_{22} & \tau_{22} &  \\
\vdots  &  &        &        & 
\end{array}\right), {\tau}_{a,a'} \sim 1-\text{Beta}(\alpha, \beta)/4
\end{equation}

I set the values corresponding to different loci to zero, as off-target probe hybridisation is unlikely to result in a signal.

\subsubsection*{Clone specific allelic variations}

The detection rate matrix, $\iota$, registers the mean clonal expression level. However, we can reasonably assume that different clones exhibit some level of diversity in gene expression. To model this deviation, I use a matrix $\gamma \in \mathbb{R}_{+}^{|s|\times|a|}$, with a LogNormal distributed prior such that $\mathbb{E}[\gamma]$ = 1.

\begin{equation} 
\gamma_{s,a} \sim \text{LogNormal}(\mu, \sigma)
\end{equation}

Regularising this prior is crucial to ensure that most of the probability density surrounds 1 by choosing a small $\sigma$. Since the matrix $\gamma$ has the same shape as the genotype matrix $\mathbf{G}$, failing to control this parameter could lead to non-identifiabilities.

\subsubsection*{Homogeneous and inhomogeneous background adjustments}
\label{sec:background_adjustment}
Raw BaSISS data are derived from biological images that include additional sources of pixel-level variability. The first source is a homogeneous additive shift $\beta \in \mathbb{R}_{+}^{|a|}$, which globally increases the expected value of a particular probe detection on the slide. This is consistent with a potential global background autofluoresence which differs in the wavelength, thus affecting each probe independently, as they are combinatorically encoded by fluorophores. In combination with the failed probe detection, this effect can lead to a global shift in the signal density. I model this effect as a Gamma distributed random variable.

\begin{equation} 
\beta_{a} \sim \text{Gamma}(\mu=0.5, \sigma=1) 
\end{equation}

The second source is an inhomogeneous background shift that adjusts for the variability in local base signal detection. Such factor should help the model explaining changes in signal composition that are not explainable by the linear combination of a known clonal genotypes. In practice, besides unknown clones, this factor tend to help with the technical noise, when some of the tissue areas are not properly registered, or in the necrotic areas, where nulcei count is high, but the \ac{BaSISS} signal is low.

Modelling the variance of this type without imposing harsh regularisation presents a challenge due to its flexibility. To represent the inhomogeneous background shift, I introduce additional pseudo-clones, denoted by $\psi$, with corresponding relative prevalence $\mathbf{M}$ and pseudo-genotype $\Gamma \in \mathbb{R}^{|p|\times|a|}_{+}$. The prevalence in this context mirrors that modelled for the actual clones. The pseudo-genotype is inferred from a Beta prior, which is manipulated to stretch the support to match the median copy number $k$ among the loci. Although I use $|\psi| = 1$, this value can be increased to allow for a more flexible background shift.

\begin{equation} 
    {\Gamma}_{p,a} \sim \text{Beta}(\alpha, \beta) \times k \\
\end{equation}

The Beta distirubtion parameters are chosen to shift the probability density towards zero, which should prevent the pseudo-genotype from being overestimated and overused by the model.

\subsubsection*{Sampling fluctuations}

I model BaSISS counts as being \explain{Negative Binomial}{is equivalent to the Poisson distribution with a Gamma prior on the mean parameter $\lambda$ \marginfig{side-plot-nb-dist.pdf}} distributed given an unobserved mean detection level $\mathbf{M}_{x,y,a}$ and over-dispersion parameter ${\alpha}_a$ which accounts for unexplained variance:

\begin{equation} 
\mathbf{D}_{x,y,a} \sim \text{NB}(\mu_{x,y,a}, \alpha_a)
\end{equation}

This is a common choice of the likelihood function \parencite{Kleshchevnikov2022-ub, Townes2023-uj}, as it is more flexible then Possion, which assumes that the variance is equal to the mean.

Over-dispersion $\alpha_a$ is sampled from Gamma distribution where distribution density is shifted towards lover value, making the likelihood Poisson like if no over-dispersion is observed.

\begin{equation}
    {\alpha}_a \sim \mathrm{Gamma}(\mu, \sigma)
\end{equation} 

\figuretextwidth{conceptual-bassis-model.pdf}{conceptual-bassis-model}
    {Mathematical modelling \ac{BaSISS} data}
    {Mathematical model for generating quantitative clone maps. The essential idea is that the \ac{BaSISS} signals count matrix $\mathbf{D}$ is decomposed into maps of clones $\mathbf{M}$ each with
    a distinct genotype $\mathbf{G}$ (grey shading), accounting for multiple sources of variability.}


This concludes main part of the model definition. The full model is conceptually visualised in \cref{fig:conceptual-bassis-model}.

\subsection{Regularisation with auxiliary constraints}
Depending on the experimental setup, we may obtain auxiliary data containing information about the state of the system we are modelling. In the spirit of Bayesian inference, propagating this information through the model should reduce uncertainty in the posterior estimation of the parameters. In this section, I describe two types of auxiliary data likely to be generated alongside the \ac{BaSISS} experiment, which can be used to regularise the model.

\subsubsection*{Bulk \acl{VAF} information}

\ac{WGS} forms the initial step of \ac{BaSISS}, and is vital for inferring clone composition and identifying potential targets for probe design. This step is carried out on the tissue sections adjacent to the block used for \ac{BaSISS}, as illustrated in \cref{fig:experiment_design}. Since the neighbouring tissue sections are likely to have similar, though not identical, clone composition, we can use the \ac{WGS} data to regularise the global clonal composition inferred by the model.

\ac{WGS} directly measures the number of reads for each allelic variant, so the relative \acf{VAF} for each locus should remain the same across tissue sections with identical relative clone composition. I apply this constraint to the model by constructing an auxiliary variable $\mathbf{VAF}^{\text{BaSISS}}$ of each mutant variant $m$ on the slide. This variable is formed by the summation of $\mathbf{M} \times \mathbf{G}$ product over spatial dimensions and represents the bulk inferred \ac{VAF} of the slide:

\begin{equation} 
\mathbf{VAF}^{\text{BaSISS}} = \sum_{x,y} \mathbf{M} \times \dfrac{\mathbf{G}^{\text{mut}}}{\mathbf{G}^{\text{wt}} + \mathbf{G}^{\text{mut}}}
\end{equation}

Next, I construct an auxiliary likelihood of $\mathbf{VAF}^{\text{BaSISS}}$ with $\alpha$ and $\beta$ parameters proportional to the number of mutated and wild-type reads found in the \ac{WGS} experiment. I incorporate parameter $u$ to account for uncertainty in \ac{WGS} data since it originates from a proximal slide that is not exactly the same as the one used in the BaSISS experiment:

\begin{equation} 
    \text{VAF}^{\text{BaSISS}}_m \sim \mathrm{Beta}(\alpha=\text{WGS}^\text{mut}_m / u + 1, \alpha=\text{WGS}^\text{wt}_m / u + 1)
\end{equation}

It is important to note that this likelihood contains exactly $|m|$ mutant alleles terms, roughly half the number of alleles $|a|$. In contrast, the main model likelihood \cref{eq:ED_full} includes $|x| \times |y| \times |a|$ terms, several orders of magnitude higher. This discrepancy means that the main model likelihood will predominantly influence the posterior estimation. Thus, I multiply the log-likelihood of the auxiliary constraint by a factor of $|x| \times |y|$ when formulating the \ac{ELBO}.

\subsubsection*{\acl{IHC} based cell type counts}

Another potential auxiliary data source stems from \ac{IHC} staining of the tissue section. \ac{IHC} staining, a standard technique in pathology, serves to identify specific cell types. Within the scope of the \ac{BaSISS} experiment, \ac{IHC} staining can be employed to characterise the spatial microenvironment of the tissue section. As the cancer cells' staining by \ac{IHC} is known, this information can be used to regularise the model.

In our study, I focused on working with \protein{CD45} \ac{IHC} staining, which pinpoints the location of immune cells. As none of the breast cancer cells should be stained by \protein{CD45}, we can establish a connection between the fraction of stained cells in the region and the cancer cell fraction. This connection is made through the construction of an auxiliary variable for cell fraction, denoted as $\mathbf{CellFrac}$, obtained by summing the clone map matrix $\mathbf{M}$ over clones of the respective types $s^-$ (including cancer cells and normal genotypes of non-immune cells) or $s^+$ (covering immune cells with normal genotypes).

\begin{equation} 
\mathbf{CellFrac} = \dfrac{\sum_{s \in s^-} \mathbf{M}}{\sum_{s \in s^+} \mathbf{M}}
\end{equation}

Subsequently, I formulated an auxiliary likelihood of $\mathbf{CellFrac}$ using the Beta distribution, where the \explain{parameters $\alpha$ and $\beta$}{The beta-binomial distribution models the number of successes in a fixed number of trials, where the probability of success follows a beta distribution. The parameters $\alpha$ and $\beta$ can be interpreted as "pseudo-counts," with $\alpha$ representing 1 + number of prior successes, and $\beta$ 1 + number of prior failures} correspond to the cell counts of \ac{IHC}- and \ac{IHC}+ stained nuclei on the slide.

\begin{equation} 
\mathbf{CellFrac}_{x,y} \sim \mathrm{Beta}(\alpha=\text{IHC}^\text{\,-}_{x,y} + 1, \alpha=\text{IHC}^\text{\,+}_{x,y} + 1)
\end{equation}

This form of regularisation proves especially beneficial in samples with very low \acl{CCF}, such as in \acl{LN} metastasis, where the majority of cells are immune cells. In these instances, the model might mistakenly interpret noise in areas with high normal cell density as cancer cell signals, thereby overestimating the local \ac{CCF}.

\subsection{Multi-sample extensions}
When conducting a BaSISS experiment on multiple tissue sections belonging to the same cancer patient, and when these sections are acquired and prepared in a batched manner, it becomes possible to share many of the model's parameters between the slides. Specifically, the shared parameters include the genotype matrix $\mathbf{G}$ (since they pertain to the same cancer case), clone-specific allelic expression variation $\gamma$ (as clones are likely to exhibit similar expression patterns), the probe confusion transition matrix $\tau$, and the expected probe detection rate $\iota$ (since the same genes are targeted, and the samples are prepared under identical conditions). The remaining parameters are specific to each of $k$ individual slides.

To address potential variations in sample preparation, I multiply the mean probe detection rate by a slide-specific probe deviation matrix $\eta \in \mathbb{R}_{+}^{|k|\times|a|}$. This matrix has a log-normally distributed prior, with $\mathbb{E}[{\eta}]$ = 1 and a variance of 0.05.

\begin{equation} 
\eta_{k,a} \sim \text{LogN}(\mu, \sigma)
\end{equation}

This extension especially pays off when certain clones are present only in one of the slides. It prevents the model from overfitting to the noise within the data.

\subsection{Inference}
\label{sec:inference}
\acl{VI} is used to approximate the posterior, specifically employing the mean-field version of \ac{ADVI} as found in the \ac{PyMC} package. As explained in \cref{sec:bayesian-intro}, \ac{ADVI} optimisation approximates the posterior distribution over unknown parameters using an appropriately transformed multivariate normal distribution. This transformation allows the exploitation of the automatic differentiation framework. Since estimating the covariance between parameters proved to be excessively costly, the optimisation is performed only on the mean and diagonal elements of the covariance matrix of the variational distribution.

Training is halted when the \ac{ELBO} no longer rises, typically after at least 15,000 iterations using the ADAM stochastic optimiser \parencite{Kingma2014-um} with a learning rate of 0.01. For subsequent analysis, the marginalised variational posterior of the parameters is employed. My experiments demonstrate that multiple initialisations may be necessary to reach an optimal solution. This requirement is primarily driven by the stochastic selection of initial values but might also be affected by \explain{numerical instabilities}{could lead to errors in gradient estimations and an erratic behaviour of the Loss function \marginfig{side-plot-elbo-bad.pdf}} occurring during inference with \ac{PyMC}.

\section{Validation}
\label{sec:bassis-validation}
Validating spatial genomics models presents a formidable challenge. Simulation studies are of limited use, as they fail to capture the complexity of real data. Furthermore, the absence of ground truth data precludes a direct comparison of model predictions with actual values. Most validation methods, therefore, rely on indirect strategies. These include assessing correlations between the distributions of conservative marker genes and inferred cell types, or evaluating the stability of the solution on technical or simulated replicates \parencite{Li2023-ik}. For the \ac{BaSISS} experiment, we generated a technical replicate to gauge the stability of both experimental and computational aspects of our approach.

\explain{Defining cell-type}{The categorisation of cell types is often ambiguous because of transcriptional plasticity. While broader cell types may exhibit stable markers, specialised or niche cell types tend to be more transient and elusive, leading to false  overclustering \parencite{Grabski2022-fz}} often poses an ambiguous task. However, the \ac{BaSISS} model seeks to map clones defined by a set of point mutations, offering a robust and often binary definition (either present or not). Consequently, it becomes feasible to generate near ground truth data using the \ac{LCM} \ac{WGS} approach \parencite{Shen2000-xj}. In this method, the tissue undergoes microdissection into regions of interest, followed by DNA extraction and sequencing. Though expensive and time-consuming, this strategy yields high-resolution allelic frequency data, allowing for an assessment of model prediction accuracy.

Technical replication and \ac{LCM}-\ac{WGS} experiments were carried out on cancer case P1, briefly introduced in \cref{sec:bassis-data-brief} and described in more detail in \cref{sec:add-later}.

\subsubsection*{Consistency of the BaSISS data}
In the initial stages of optimising the \ac{BaSISS} experimental protocol, we produced two consecutive slide technical replicates over several years. These replicates were created using different probe designs, imaging platforms, and tissue preparation strategies, as detailed in \cref{sec:bassis-protocol-supplementary}. This sequence offers an invaluable resource for evaluating the consistency of the experimental protocol, particularly concerning signal distribution.

\figuretextwidth{basiss-replication-stats.pdf}{basiss-replication-stats}
    {Technical consistency of the \ac{BaSISS} protocol}
    {\emph{Top Left} part of the plot shows the Person's correlation between the \ac{VAF} distributions of the two technical replicates - R0 and R1. Data are presented as mean
    estimates and 95\% HPDI. \emph{Bottom Left} confirms the correlation between \ac{WGS} read normalised \ac{BaSISS} mutant (solid) and wild-type (hole) allele counts. This sistematically confirms the correlation between \ac{BaSISS} signal density and \ac{CNA} level. \emph{Right} part of the plot shows the fields $\mathbf{M}$ inferred by the model for each of the two replicates. The fields are visually similar, considering tissue distortion and distance on z-stack. Scale bar = 2.5 mm}

\cref{fig:basiss-replication-stats} displays the correlations between the \ac{VAF} distributions of the two technical replicates. Despite the variations in experimental conditions, the \ac{BaSISS}-derived \ac{VAF}s demonstrate strong correlation across replicate experiments on serial tissue sections (R = 0.76–0.93, Pearson's). This result underscores the \ac{BaSISS} protocol's robustness against changes in experimental circumstances.

I have previously addressed the visual correlation between \ac{BaSISS} signal density and \ac{CNA} level in \cref{sec:bassis-data-brief}, citing it as a motivation for the model design. Nonetheless, it is crucial to formally evaluate whether this correlation holds generally across loci. We possess \ac{WGS} data for the samples, providing us with bulk \ac{WGS} estimates. Although the precise clonal composition remains unknown, we can reasonably presume that normalising \ac{BaSISS} signals by \ac{WGS}-detected reads should account for the compositional differences between samples. As depicted on the bottom-left subplot of \cref{fig:basiss-replication-stats}, this assumption holds true, with $\log_2$(BaSISS/WGS) values showing reasonable correlation (R = 0.75–0.9, Pearson’s).\footnote{Writing thesis is actually useful; in our publication, I reported this equation incorrectly as \ac{VAF}s instead of the raw signal counts.} This finding supports the assumption that \ac{BaSISS} signal density correlates with the number of DNA copies in the sample.

\subsubsection*{Consistency of the model predictions}
The replication experiment mentioned earlier serves to assess the stability of the model's predictions. As the probes are engineered to target identical mutations, the model ought to generate comparable clonal maps for both replicates. However, certain variables, such as the probe detection rate, denoted as $iota$, may vary due to differences in probe barcodes and experimental conditions. When run with identical hyperparameters on the two replicates, the model yields reasonably similar clonal maps, accounting for the distance in the z-stack between them (refer to \cref{fig:basiss-replication-stats}).

\figuretextwidth{basiss-lcm-validation.pdf}{basiss-lcm-validation}
    {\ac{LCM}-\ac{WGS} validation of the \ac{BaSISS} model prediction}
    {\emph{Top} part of the plot shows the Person's correlation between the \ac{VAF} distributions of the two technical replicates - R0 and R1. Data are presented as mean. Scale bar = 2.5 mm. \emph{Bottom} shows \ac{VAF} values for the validated regions: \ac{BaSISS} signal, which appears noisy, \ac{BaSISS} model imputed \ac{VAF} highly concordant with \ac{LCM}-\ac{WGS} estimates.}

\subsubsection*{Validation of model predictions with \ac{LCM}-\ac{WGS}}
To apply our work to spatial characterisations, we must validate the accuracy of our detailed predictions concerning clonal compositions. Characterising the entire tissue slide is unfeasible due to its laborious nature, but we can select a small number of regions for \ac{LCM}-\ac{WGS} to confirm the model's predictions. For this purpose, we performed \ac{LCM}-\ac{WGS} on the residual tissue blocks.

It is vital to acknowledge that the tissues used were not adjacent slides but were located at a significant distance along the z-stack. This distance represents an unfortunate oversight in our experimental design. We located areas with structural correspondence, excised them, and then conducted sequencing. Following this, we compared the resulting \ac{VAF} values to both the raw \ac{BaSISS} signal \ac{VAF}s and the model predictions formulated for the corresponding regions (refer to \cref{fig:basiss-lcm-validation}).

The contrast between the noisy raw \ac{BaSISS} signal VAFs and the nearly exact matching model predictions with the \ac{LCM}-\ac{WGS} estimates is striking. This contrast attests to the model's capability to precisely forecast the spatial clonal composition of the tissue. Furthermore, it affirms the model's suitability for downstream data analysis.

\section{Model assumptions and outlook}
\label{sec:basiss-model-assumptions}

Quantitatively estimating the arrangement of genetically different clones in space, using information from their genes obtained through \ac{ISS}, is a difficult task. The model described here addresses the multiple sources of biological and technical noise to generate clonal maps, which have been orthogonally validated (see \cref{sec:bassis-validation}). However, the design of any model inevitabely introduces assumptions that must be considered during result interpretation. This section discusses the assumptions and limitations of the \ac{BaSISS} model as well as possible future directions.

\subsubsection*{Signal binning}
The \ac{BaSISS} protocol relies on dividing \ac{ISS} signals into regions, a process known as binning. This binning is convenient, as it reduces the number of parameters, enabling a straightforward application of the Negative Binomial likelihood function. The decision to bin is driven by the fact that the \ac{BaSISS} signals are sparse; however, this leads to information loss. Although this trade-off suits the current stage of \ac{BaSISS} experimental technology, it may become less justified as the technology evolves and improves.

Mathematical frameworks exist that allow the model to engage directly with the point signals. For example, both Markov Random Fields \parencite{Petukhov2022-pv} and spatial Poisson processes \parencite{Qian2020-mp} have been successfully employed to connect spots to the latent fields. Alternatively, the Integrated Nested Laplacian Approximation (INLA) \parencite{Rue2009-an} of the latent \ac{GP}, although not yet popular in spatial genomics, appears promising.

I foresee that, as experimental technology advances, the \ac{BaSISS} model can be extended to incorporate these approaches in the future.


\subsubsection*{Spatial correlation}
The model employs a \ac{GP} prior to handle the sparsity of the \ac{ISS} data, assuming a smooth change in clonal composition across space. This assumption stems from the observation that breast cancer tissue maintains structural integrity, causing cells to remain in proximity after division. However, the model fails to consider detached compartments like \explain{ducts}{in breast cancer provide a rigid border that prevents cancer cells from spilling out, yet the model incorrectly propagates information from the neighbouring green clone, leading to incorrect assumptions about the genetic status of the surrounding normal cells \marginfig{side-plot-duct-overflow}}, leading to potential errors at the interfaces between compartmentalised regions. Consequently, the inferred clonal composition at these borders might appear more intermixed than in reality. 

\subsubsection*{Copy number variation and expression}
The \ac{BaSISS} protocol detects RNA, while the model operates with clone genomes. To associate genetic clones with \ac{BaSISS} data, I assume a positive linear relationship between an allelic variant's copy number and its expression. The correlation of \ac{BaSISS} probes, normalised by \ac{WGS} across the examined sections, endorses this assumption (see \cref{sec:bassis-validation}). Previous studies also support this relationship \parencite{Handsaker2015-jx}, though exceptions do exist.

For instance, genes such as \gene{TP53} and \gene{MYC}, as demonstrated by \textcite{Shao2019-vq}, appear to deviate from the linear trend. This deviation may arise from dosage compensation mechanisms that can assume a nonlinear form. An example of this can be seen in \gene{MYC} expression, which seems to be resistant to low copy number alterations but exhibits high expression in focal high-level amplifications \parencite{Schukken2022-jm}. These observations urge careful consideration during the design of allele target panels.

\subsubsection*{Homogeneity of clonal expression}
The model assumes that alleles of a particular clone are expressed homogeneously across the slide. Potential violations might be partly compensated by multiple defining alleles of clonal branches, or by considering an inhomogeneous background (\cref{sec:background_adjustment}). The former might counterbalance the effects of highly variable alleles on the \ac{ELBO} loss via overdispersion parameter, while the latter might capture some heterogeneity as pseudo-clones. Hence, the spatial distribution of the pseudo-clone factor may highlight problematic regions. 

\subsubsection*{Phylogenetic tree}
Since the clonal genotype matrix or phylogeny is fixed, the model cannot adjust it directly. Though selecting an \explain{optimal tree solution}{Original idea was to write the tree sampler and compare \ac{ELBO}s of the fitted models as the model selection criteria. This practice is justified in \textcite{Cherief-Abdellatif2018-fv}} based on BaSISS data is theoretically feasible, in practice, various tree solutions yield similar \ac{ELBO} values. This likely results from latent parameters causing practical unidentifiability. Attention should, therefore, be given to \ac{WGS} mutation clustering and copy number estimation to avoid errors that may produce unrealistic clonal abundances and growth patterns. However, assessing model residuals could highlight problematic alleles and indicate the feasibility of the genotype matrix.

\subsubsection*{Confidence intervals of the inference}

The mean-field variational approximation offers a cost-effective means of inference, yet it comes with inherent limitations. Its first limitation arises from the aim of optimising KL divergence $KL(Q||P)$. If the true posterior $P$ displays multimodality, the variational approximation $Q$ fits only one mode, consequently overlooking other viable solutions. An alternative definition of the \explain{reverse KL divergence}{\marginfig{side-plot-KL-PQ.pdf} In the case of a non-alternative selection between two opposing entities, both will manifest suboptimally} might strive to encompass all modes, but it often proves undesirable, as the approximate distribution may assign high density to regions where the true posterior is improbable. Additionally, the mean-field approximation typically underestimates the parameter variance \parencite{Kucukelbir2016-gn}. Regrettably, utilising full-rank \ac{VI} or \ac{MCMC}, which provide more accurate variance estimations, remains computationally impractical for the \ac{BaSISS} model. Nevertheless \explain{technology development}{\marginfig{side-plot-gpu-trans.pdf} Number of transistor per GPU chip has an exponential trend \parencite{Bryant2022-yq}} in a field of high performance numerical computations and the use of more powerful hardware resources might make these approaches feasible in the future. 