\chapter{Bayesian Model of In situ Sequencing Data}

\section{Introduction}

Lineage tracing through somatic mutations can reveal ancestral connections between cancer subclones. Yet, current methods fall short in preserved human tissue studies \citep{Yates2015-eg,Jamal-Hanjani2017-ed,Jones2008-tg,Shah2009-lx,Casasent2018-gx,Tarabichi2021-xx}. Techniques like laser capture microdissection (LCM) combined with limited nucleic acid library sequencing, including single-cell sequencing, help to some extent in delineating subclone spatial structures \citep{Shen2000-xj,Casasent2018-gx}. However, they don't effectively capture the diversity of cancer clones across entire tumour sections. While recent spatial genomics techniques can identify cancer clones based on distinct copy number profiles \citep{Zhao2022-dp,Erickson2022-mq}, they fall short in detecting point mutations or quantifying mixed clones.

Though in situ hybridization \citep{Janiszewska2015-kb} and mutation-specific padlock probes \citep{Larsson2010-bp,Grundberg2013-te,Ke2013-km,Baker2017-jz} can detect individual mutations in situ, their potential is limited by the number of available fluorophores, allowing them to trace only a few mutations at once. Considering the genetic uniqueness of each cancer and its subclones, tracing multiple specific somatic mutations is crucial to decipher spatial and temporal ancestral relationships \citep{Nik-Zainal2012-tt}. The solution lies in encoding in situ sequencing probes combinatorially, allowing them to be captured with a multi-round imaging strategy, thus enabling multiplex \ac{BaSISS}.

The \ac{BaSISS} protocol uses fresh frozen tissue blocks, which are serially cryosectioned. This produces tissue for both bulk \ac{WGS} and z-stacked sections for spatial clone mapping. After identifying subclones from bulk WGS, \ac{BaSISS} employs two main steps. First, it uses padlock probes designed for both mutant and wild-type alleles, marked with a unique 4–5 nucleotide barcode for multiplexing \citep{Ke2013-km}. These probes can target any expressed somatic mutation, including rearrangement breakpoints, and can be supplemented with copy number alterations. Next, \ac{BaSISS}  uses a cyclical microscopy approach, akin to gene expression in situ sequencing protocol \citep{Ke2013-km,Svedlund2019-xb}.

The procedures described, along with initial data processing, produce two primary sets of information. \ac{WGS} reveals anticipated clonal expansions, identifies point mutations and copy number variations specific to each clone. The spatial component locates alleles identified by the \ac{WGS} in the tissue section. We also obtain a proxy to determine cell locations through segmented nuclei locations. Nevertheless, the interpretation of subclone locations becomes complicated due to the inherent complexity of the cancer genome, tissue heterogeneity, and potential systematic errors in the technology. These challenges drive the need for a robust statistical approach.

\section{Popular approaches in modelling spatial transcriptomics data}

Spatial inference is a common challenge spanning various research fields such as economics, geology, and ecology. Consequently, the field of spatial statistics has become rich in tools and methodologies, often finding applications well beyond their original purpose. \acl{GP}, for instance, once used solely in geostatistics, now find use in diverse areas such as optimization problems, finance, and astronomy. This cross-applicability of tools is indicative of the universal nature of spatial problems. Even in spatial genomics, a relatively recent field, there exists a wide array of existing mathematical tools that can be applied to solve its unique challenges.

While this overarching framework of spatial statistics provides many general tools, each field inevitably cultivates its own preferred methodologies. These are often dictated by the specific nature of the data and the underlying scientific understanding. In spatial genomics, the uniqueness arises from both the type of data generated and the biological comprehension that guides the analysis. This leads to inquiries that concern mapping and characterising cells using specific markers and observed spatial distributions. Such tasks are often carried out using latent variable modelling, where observations (expression) are explained by unobserved common factors (cell types/programs), as represented by:

\begin{equation}
D_i \sim f(k_i)
\end{equation}


When dealing with spatial experimental approaches that have low spatial resolution, such as spatial transcriptomics, data is commonly presented as an expression matrix (location x gene). However, for methods with higher signal density and resolution, the initial spots are often aggregated into cells. This can be done directly using segmentation masks obtained from nuclei or visualised cell membranes (when the experimental setting permits), or by inferring cell borders directly from the signal distribution.

One crucial consideration is that if the factors are unconstrained, there is no guarantee that the lower-dimensional representation  $\mathbb{R}^k$ will resemble the actual cell types. This leads to the use of constraints, applied either singly or in combination. These constraints might be spatial, employing tools like \ac{GP}, Markov random fields, or conditional autoregression. Alternatively, the latent space can be constrained by harnessing the complementary nature of \ac{scRNAseq} data. This involves learning a joint latent space representation from both scRNAseq and spatial data, or extracting signatures from \ac{scRNAseq} data to guide the spatial model. The latter approach, commonly referred as cell type deconvolution\footnote{In my opinion a more apropriate name is cell type decomposition, since there are no explicit convolution operation involved}, where data is decomposed into a set of predefined factors has been especially popular recently, which brought to existence a zoo of methods utilising many possible architectures.

Among the various models, recent benchmarking has revealed that generative Bayesian models perform well in the task of cell type decomposition. Considering additional favourable properties such as interpretability and direct uncertainty estimation, this class of models often becomes an appealing choice for modelling tasks within the field.

\section{Bayesian statistical modelling}

In many practical contexts, we find ourselves in situations where the underlying mechanisms governing the system are hidden or only partially observed, and the observations are noisy and limited. Such scarcity of information introduces inherent uncertainty in our understanding of the system. It is within this landscape of uncertainty that the Bayesian understanding of probabilities emerges as a natural tool. Rather than mere frequencies, probabilities are construed as representations of the degree of belief, capturing the epistemic uncertainty about the system parameters. A statistical model, crafted within this Bayesian paradigm, acts as a bridge between the observed data and the underlying phenomena, translating our assumptions and knowledge about the problem into a coherent probabilistic framework. This in turn allows us to make predictions, perform inference, and understand more deeply the structure of the data.

Given a set of observations ${D} = \{t_1, t_2, … t_n\}$, a statistical model aims to describe how these observations are generated. One can consider these observations a realisation of a random variable $T$ which is controlled by a set of parameters $\theta$. Therefore, a statistical model could formally be defined by specifying the joint probability distribution $p({D}, \theta)$, describing the stochastic relationship between the observations and the controlling parameters. We can rewrite the joined probability using conditional probability, leading us to the Bayes' theorem equation:

\begin{equation}
p(\theta | {D}) = \frac{p({D} | \theta) p(\theta)}{p({D})} = \frac{p({D} | \theta) p(\theta)}{\int p({D} | \theta) p(\theta) d\theta}
\end{equation}

Here $p({D}|\theta)$ is the likelihood, representing the probability of the observations given the parameters; $p(\theta)$ is the prior distribution over the parameters, reflecting our beliefs about $\theta$ before observing the data; $({D}| \theta)$ is the posterior distribution after observing the data; and $p({D})$ is the evidence, obtained by marginalising $\theta$ out of the joined distribution. It is essentially a normalising constant, ensuring that posterior is a valid probability distribution and integrates to one. 
Both frequentist and Bayesian models rely on the likelihood $p({D}|\theta)$, but they interpret the parameters $\theta$ differently. In the frequentist perspective, $\theta$ is fixed and its confidence intervals are estimated by considering multiple repeated samples of ${D}$. In the Bayesian view, $\theta$ varies for each observation set of ${D}$ and uncertainty in its estimation is expressed as a probability distribution. With this understanding, we can view statistical inference as an update on our prior belief of the distribution of hidden parameters based on additional evidence.

\begin{equation}
\text{posterior} \propto \text{likelihood} \times \text{prior}
\end{equation}

To demonstrate, let’s consider a simple gamma-poisson model of the gene expression, with the gene expression rate as a hidden variable. Consider an observation $D_n$  of observed counts in $n$ cells. I will model the gene expression count for each cell $i$ as a $\text{Poisson}$ random variable with the rate parameter $\lambda$ which has a $\text{Gamma}$ prior, where $\alpha$ and $\beta$ are the hyperparameters shaping our initial belief about expression rate.

\begin{align}
    D_i & \sim \text{Poisson}(\lambda) = \frac{\lambda^{D_i} e^{-\lambda}}{D_i!} \\
    \lambda & \sim \text{Gamma}(\alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha - 1} e^{-\beta \lambda}
\end{align}
    
Using the fact that the likelihood is independent Poisson distributions and the prior is a Gamma distribution it is trivial to derive the analytical form of the posterior $p(\lambda|D)$:

\begin{align}
        p(\lambda|D) & \propto p(D|\lambda) p(\lambda) \\
         & \propto \prod_{i=1}^N \frac{\lambda^{D_i} e^{-\lambda}}{D_i!} \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha - 1} e^{-\beta \lambda} \\ 
        & \propto  \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\left(\sum_{i=1}^N D_i\right) + \alpha - 1} e^{-\lambda(n+\beta)} \\
         & = \text{Gamma}(\alpha + \sum_i^n D_i, \beta + n)
\end{align}

The graphical depiction of the components is shown on \textbf{FIGURE N}.


The process of inference, while appearing simple in principle, presents substantial difficulties in actual computation. A major obstacle lies in the denominator of the Bayesian formulation, the integral often becomes intractable  thus rendering even moderately complex Bayesian models intractable, apart from a few cases with carefully selected \explain{ \emph{conjugate priors}}{Write a simple explanation} like shown in the previous example. This issue is a significant constraint in the applicability of Bayesian models, which must be carefully considered in their practical implementation and model formulation.

The solution to the problems came through a sequence of developments in \ac{MCMC} methods, including the Metropolis algorithm \citep{Rosenbluth1953-gu}, Metropolis-Hastings algorithm \citep{Hastings1970-mb}, Gibbs sampling \citep{Geman1984-nk}, and Hybrid Monte Carlo (HMC) \citep{Duane1987-gu}. These methods permit the generation of samples from the posterior probability distribution by forming an invertible Markov chain. The equilibrium distribution of this chain corresponds to the targeted posterior distribution \citep{Roberts2004-op}. Consequently, if one were to compute the constructed Markov chain, a guarantee exists for the convergence to the genuine posterior distribution of the model, given that the number of sampling steps approaches infinity. In practical terms, provided the model is appropriately specified, convergence tends to occur in a sensible duration. The samples derived from the converged chain may then be utilised to estimate the requisite summary statistics. 

\ac{MCMC} has revolutionised the field of Bayesian inference, segregating the procedures of model formulation and inference. It must be noted, however, that these methods do not scale with ease when faced with the dimensions and complexity of the model. They often either demand an extensive number of steps or involve calculations that, although fewer in number, are considerably computationally demanding. In scenarios where the analytical posterior is inaccessible, and the computational cost of \ac{MCMC} is substantial, it is common to use alternative approximation schemes such as \ac{VI}.

\acl{VI} is framed as an optimization problem, where the objective is to identify a distribution within a constrained family of tractable distributions that most closely approximates the target distribution \citep{Blei2017-dm}. This technique can be seen as an optimization problem, where the goal is to find a distribution that is most similar to the target distribution, within a constrained family of tractable distributions. Mathematically, the problem of \acl{VI} can be formulated as finding the distribution $q(\theta)$ of the variational family $\mathcal{Q}$ that is following the true posterior $p(\theta | D)$ as closely as possible. The common distance measure that is used is the Kullback-Leibler (KL) divergence, thus the goal is:

\begin{equation}
q(\theta) = \argmin_{q \in \mathcal{Q}} \text{KL}(q(\theta) || p(\theta | D))  
\end{equation}

After expanding KL divergence and rewriting conditional probability:

\begin{align}
q(\theta) &= \argmin_{q \in \mathcal{Q}} \int q(\theta) \log \frac{q(\theta) p(D)}{p(\theta, D)} d\theta \\
&= \argmin_{q \in \mathcal{Q}} \int q(\theta) [\log q(\theta) + \log p(D) - \log p(\theta, D)] d\theta \\
&= \argmin_{q \in \mathcal{Q}} \mathbb{E}_q [\log q(\theta) -  \log p(\theta, D)] + \log p(D)
\end{align}

Notice that $\log p(x)$ is constant with respect to $q$, so we can ignore it when minimising KL divergence. Now we can define the optimisation objective, which is the inverse expectation term, referred as the \explain{Evidence Lower Bound}{Compose a simple illustration on why this is the Evidence LOWER bound} (ELBO). Hence, the original problem of minimising KL divergence between approximation distribution and posterior becomes equivalent to the maximisation of ELBO. 

One significant development in \ac{VI} is \ac{ADVI} \citep{Kucukelbir2016-gn}. \ac{ADVI} generalises the approach by employing the normal family of distributions as the guide, thereby enabling the application of gradient-based optimisation techniques, via \explain{reparameterization trick}{Explain what is the reparameterization trick}.

In \ac{ADVI}, the variational distribution is assumed to be multivariate normal with an appropriate transformation.
\begin{equation}
q(\theta) = f(\mathcal{N}(\mu, \Sigma))
\end{equation}

Here, $\mu$ and $\sigma$ are the optimised mean and covariance parameters, and $f$ is a \explain{transformation function}{Consider giving an example} that maps the normal distribution to the desired distribution. The transformation function is chosen such that the support of the transformed distribution is the same as the support of the target distribution. For example, if the target distribution is a Gamma distribution, then the transformation function is the Exponential function. 

In practice, additional constraints are imposed on the covariance matrix, considering it either low-rank or simply diagonal (mean-field) \citep{Blei2017-dm}. Although these assumptions may overlook a probable covariance structure between model parameters, and frequently underestimate the variance, they dramatically decrease the number of parameters that can be optimised. This makes the inference of even larger models feasible.

In addition to advancements in inference algorithms, contemporary machine learning frameworks like \ac{Theano}\footnote{A discontinued python framework that I had a misfortune to use as a backand for model optimisation defined in PyMC package. Currently a fork is maintained under the name Aesara}, \ac{PyTorch}, \ac{TensorFlow}, and \ac{JAX}, have notably improved the efficiency of Bayesian inference. This is particularly true for methods that utilise automatic differentiation, such as the Hamiltonian Monte Carlo \ac{MCMC} family techniques and \ac{ADVI}. Probabilistic frameworks, including \ac{PyMC}, \ac{Pyro}, and \ac{NumPyro}, are simplifying the Bayesian model development process. By managing most of the underlying calculations, these frameworks allow researchers to concentrate more on conceptual model development and spend less time on numerical details.

\ac{BaSISS} profiles extensive areas of tissue, encompassing hundreds of thousands of cells. The underlying processes that generate the final data are complex. Therefore, defining a statistical approach becomes unfeasible without the utilisation of cutting-edge inference approaches and frameworks capable of handling this complexity.

\section{Methods}
\subsection{BaSISS data characterisation}
Before diving into the model design, let's first examine the raw \ac{BaSISS} data for three samples from P1 (P1-ER1, P1-ER2, and P1-D1). A comprehensive description of the samples and the experimental procedure is discussed in \ref{sec:basiss_data}. In brief, the multi-region \ac{WGS} identified six clonal expansions, and 51 padlock probes were designed to track them on tissue slides targeting mutant and wildtype allelic variants. In total, these three samples contain approximately 1,000,000 cells, spanning 245 mm\(^2\) of tissue (Table \ref{tab:basiss_data_raw}), and roughly 1,000,000 detected \ac{BaSISS} signals, 25\% of which target mutant variants.

If one is to colour allelic variants by their clonal origin, it is easy to note that the data has hierarchical structure (Figure \ref{fig:basiss_data_raw}). For example private mutations of \green{green} and \blue{blue} clones tend to colocalise, while avoiding \orange{orange} clone private mutations. This is consistent with the clonal phylogeny, where \green{green} is a descendent of \blue{blue}, and therefore possesses all \blue{blue} mutations, while \orange{orange} clone is a separate lineage. When it comes to the corresponding wild type allels the situation is reversed, in the suspected location of \green{green} clone, wild type variants of the genes where it has a mutation is depleted, which makes sense as the copy number of the wild type variant is decreased once mutation happens. An even more obvious relation between copy number and singal density is observed for the copy number variable locus of \gene{FGFR1}. Since \orange{orange} clone possesses a greater copy number gain of 12 in this locus, the signal density is increased in the corresponding region  (Figure \ref{fig:basiss_data_raw_zoom}). 

These observations motivate two key model design decisions. First of all, it is obvious that we won't be able to extract single cell level information from the data, as the signal density is too low. Therefore, we will be working with aggregated data, where the unit of observation is a region, not a cell. Cells inside these regions can belong to different clonal lineages and each should contribute to the observed signal count. Secondly, there is a clear relation between the copy number and signal density, which suggests that we should incorporate copy number information into the model.

\subsection{Non-negative matrix factorisation}
As discussed previously, the \ac{BaSISS} protocol involves recording a series of fluorescent spots, each of which corresponds to a targeted allele. These spots are decoded into a class of different barcodes $A$, resulting in a table of tuples $(a_i, x_i, y_i)$, where $a_i$ represents the allele of spot $i$, and $x_i$ and $y_i$ denote its respective two-dimensional coordinates on a grid.

This grid can be visualised as a three-dimensional array $\mathbf{D} \in \mathbb{N}^{|a| \times |x| \times |y|}$, where $a$ refers to the specific allele, and $x$ and $y$ correspond to coarse-grained coordinates on a regular grid of dimensions $|x| \times |y|$. The grid size was selected to be 108.8 $\mu$ m, a choice that takes into consideration a balance between data sparsity, precision, and computational cost.

The essential concept behind this approach is that the expected number of \ac{BaSISS} signals can be decomposed into maps of $|s|$ distinct clones $s$, each represented as $\mathbf{M} \in \mathbb{R}^{|s| \times |x| \times |y|}$, and a genotype $\mathbf{G} \in \mathbb{N}^{|a| \times |s|}$,

\begin{equation}
    \mathbb{E}[\mathbf{D}] \approx \mathbf{G} \times \mathbf{M} = \sum_{s} \mathbf{G}_{a, s} \mathbf{M}_{s, x, y}
    \label{eq:ED}
\end{equation}

The genotype matrix $\mathbf{G}$ encapsulates the information about the number of each allelic copy present in each clone $s$. Essentially, $\mathbf{G}$ serves as a matrix representation of the underlying phylogenetic tree, providing a detailed guide to the allelic configuration at each branch of the tree. 

The maps $\mathbf{M}$, on the other hand, offer insight into the relative prevalence of each clone in a specific area of the grid. The maps are modelled as non-negative matrices, as the number of cells in a given region cannot be negative. The non-negativity constraint is also biologically relevant, as it is impossible for a clone to have a negative prevalence in a given region.

Since all three matrices are non-negative, the problem can be categorised as a \emph{non-negative matrix factorisation} problem. It is a known characteristic of such problems that they typically do not have unique solutions. Therefore, additional constraints, guided by biological reasoning, are required to ensure the identifiability of the solution. While the matrix $\mathbf{G}$ possesses a clear biological interpretation and is essentially fixed, the spatual nature of matrix $\mathbf{M}$, being a latent variable, must be carefully defined.

\subsection{Laten Gaussian Process}

Since the matrix $\mathbf{M}$ is designed to represent a spatially continuous map, we can utilise the inherent assumptions about spatial locality present within the data. Specifically, it is logically consistent to anticipate that points that are spatially closer to one another should display higher similarity than those further apart. This expectation not only arises from a general consideration of spatial smoothness but also finds roots in biological rationale. Considering the process of \explain{cancer growth}{Maybe ad a cartoon}, for example, dividing cells are typically situated near their parent cells, which further accentuates local similarity. This concept of spatial locality can be mathematically articulated using a Radial Basis kernel function:

\begin{equation}
    K_x(x,x') = \exp\left[ -\dfrac{(x-x')^2}{2l^2}\right]
\end{equation}

Here, $x$ and $x'$ denote spatial coordinates, and $l$ is a length scale hyperparameter that governs the smoothness.

We can naturally express a generative process constrained by the kernel function through the \acf{GP}. The \ac{GP} represents a stochastic process where any finite set of random variables exhibits a joint Gaussian distribution. Put simply, a \ac{GP} serves as a distribution over functions which shape is constrained by the kernel. 

The \ac{GP} relies on two main components for its specification: a mean function $m(x)$ and a covariance function. Since we are dealing with a two-dimensional spatial map, where $x$ and $y$ dimensions are independent, the covariance function is defined as the product of two one-dimensional kernels, $K_{xy} = K_x \times K_y$. As there is no prior knowledge about the expected prevalence of clones in the region, the mean function is set to zero:

\begin{equation}
    m_s(x,y) \sim \mathcal{GP}(0, K_x \otimes K_y)
\end{equation}

The \ac{GP} is a distribution over functions that belong to the $\mathbb{R}$ domain. However, 
we want $\mathbf{M}$ to be non-negative, and even more specifically, to be a probability simplex. To achieve this, we apply a \emph{softmax transformation} to the \ac{GP} output, which maps the real-valued function to a probability simplex. The softmax transformation is defined as follows:

\begin{equation}
    \mathbf{M} = \left[ \dfrac{e^{\tau m_0}}{1 + \sum_{i=0}^{|s|-1}{e^{\tau m_i}}}, \dots, \dfrac{e^{\tau m_{|s|-1}}}{1 + \sum_{i=0}^{|s|-1}{e^{\tau m_i}}} \right]
\end{equation}

Here, $\tau$ is a temperature hyperparameter that controls the prior expectation of clone intermixing as shown on the (Figure \ref{fig:softmax_GP}). By setting high \explain{$\tau$ values}{Explain that for different number of components, tau should be chosen independently}, we express our beliefe that clones are likely to be spatially separated, while higher values of $\tau$ indicate a higher degree of intermixing. Since I didn't have a strong expectation about the degree of intermixing, I set $\tau$ = 2 for the case with 6 clones, which corresponds to a weak belief that clones tend to intermix.

The use of \ac{GP} as a prior for the spatial maps $\mathbf{M}$ is a convinient way to deal with the sparcity of the data observed in the \ac{BaSISS} data. It allows to propagate information over the spatial dimensions in a \explain{non-parametric}{GP considered non parametric, explain why} way, stabilising the solution and reducing the impact of sampling fluctuations. Nevertheless, the model would benefit from excplicit parametrisation of the known sources of variation.

\subsection{Sources of variation}

I model the expected number of \ac{BaSISS} signals, denoted by $\mathbb{E}[\mathbf{D}]$, within a specific region. This calculation involves two components: the genotype matrix $\mathbf{G}$ and the spatial map $\mathbf{M}$. The matrix $\mathbf{G}$ encodes integer values representing the allelic copy number within each cell of a particular clone, while the spatial map $\mathbf{M}$ shows the relative prevalence of each clone in the region.

The product of $\mathbf{G}$ and $\mathbf{M}$ provides a value that one might interpret as the average allelic copy number in the given region. However, this interpretation differs significantly from the expected number of BaSISS signals: the fluarophores observed as a product of sequencing by ligation of the amplified padlock probes hybridised with cDNA reads synthesised from the mRNA expressed by cells in the region. Hence, in order to rescale the product of $\mathbf{G}$ and $\mathbf{M}$ to the expected number of BaSISS signals, we need to account for the following sources of variation:

\begin{enumerate}
    \item Cellular density $\nu$
    \item Differential probe specificity $\iota$
    \item Allelic confusion $\tau$
    \item Clone expression variations $\gamma$
    \item Homogeneous and inhomogeneous background adjustments $\beta$
    \item Overdispersed sampling fluctuations $\alpha$
\end{enumerate}

Accounting for these, the equation for the expected number of BaSISS signals \eqref{eq:ED} becomes:

\begin{equation} 
\mathbf{M}_{a, x,y} = \underbrace{\nu_{x,y}}_{\text{cell density}} \cdot \overbrace{\iota}_a^{\mathclap{\text{detection rate}}} \cdot \underbrace{\sum_{a'} \tau_{a,a'}}_{\text{probe confusion}} \sum_s  \overbrace{\gamma_{s,a}}^{\mathclap{\text{clone-specific expression}}} \underbrace{\mathbf{G}_{a,s} \mathbf{M}_{s,x,y}}_{\text{clone contribution}} + \overbrace{\beta_a}^{\mathclap{\text{background}}}
\label{eq:ED_full}\\
\end{equation} 

Let's discuss each of these components in more detail.

\subsubsection*{Cellular density}
The cellular density $\nu_{x,y}$ directly influences the number of observed signals in a given region. While this density represents the expected number of cells, directly observing it poses a challenge. Some tools attempt to infer cellular density from transcriptional signals alone \citep{Kleshchevnikov2022-ch}. However, this method is unreliable due to \explain{confounding expression}{same singal level could be explained by higher global expression, or more dense cell populations} specific factors.

Fortunately, the number of nuclei in the region provides necessery information to disentangle the effect of cellular density from the other sources of variation. We obtain this data during the \ac{BaSISS} signal acquisition process. Using a neural network model, we segment the DAPI stained image of nuclei, saving the nuclei counts in a two-dimensional array $\mathbf{N} \in \mathbb{N}^{|x| \times |y|}$. These counts may not always accurately reflect cell densities. This inaccuracy can result from segmentation errors or because the slide captures only a fraction of the cells. As a result, we model the nuclei counts $\mathbb{N}_{x,y}$ as Poisson distributed values. Their mean, $\nu_{x,y}$, represents cell densities and follows a weak Gamma prior.

\begin{equation}
    \mathbf{N}_{x,y} \sim \text{Poisson}({\nu}_{x,y})
\end{equation}
\begin{equation}
    {\nu}_{x,y} \sim \text{Gamma}(\mu, \sigma)
\end{equation}

The prior parameters $\mu$ and $\sigma$ are set to a weekly informative distribution center around the expected number of cells in the region.

\subsubsection*{Differential probe specificity}

\ac{BaSISS} ability to detect allelic variants relies on a multistage process of reverse transcription, padlock probes annealing, amplification and fluorophore detection \citep{Svedlund2019-xb}. As it is hard to model each of the steps explicitly, we model the overall detection rate $\iota \in \mathbb{R}^{|a|}_{+}$ for each allele $a$ as a Gamma distributed random variable.

\begin{equation}
    {\iota}_{a} \sim \text{Gamma}(\mu, \sigma)
\end{equation}

The prior parameters $\mu$ and $\sigma$ are set to a weekly informative distribution with the probability density shifterd towards zero, as we do not know the average probe specific detection rate, but it is likely to be low given that we observe < 1 \ac{BaSISS} signal per cell.

\subsubsection*{Allelic confusion}

The difference between wild type and mutated allelic variants often amounts to a single nucleotide. This elevates the chances for the padlock probe to anneal to the off-target allelic variant, resulting in \explain{allelic confusion}{Draw how it looks}. Though DNA ligases are optimised for fidelity, they can make mistakes, particularly under suboptimal conditions \citep{Lohman2016-ec}. This mistake-prone behaviour particularly contaminates mutated alleles. Since most cells contain at least two wild type copies, this creates spurious noise, as seen in \ref{fig:allelic_confusion}.

To model this effect, I design a sparse transition matrix ${\tau} \in \mathbb{R}^{|a|, |a|}_{(0,1)} $, populating it with transition probabilities $ \{ {\tau}_{a,a'}\} $ for each allele. I impose a strong regularising Beta prior on $ \tau_{a,a'} $, shifting the probability density close to 1 and preventing it from dropping below 0.75. This restriction avoids non-identifiabilities and is a reasonable constraint, given that this effect is selected against during protocol optimisation.

\begin{equation}
 {\tau} = \left(\begin{array}{c|cccc}
       & \ldots & \text{wt} & \text{mut} & \ldots \\ 
\hline
 \vdots &  &        &        & \\
\text{wt}      &  & \tau_{11} & 1-\tau_{11} &  \\
\text{mut}     &  & 1-\tau_{22} & \tau_{22} &  \\
\vdots  &  &        &        & 
\end{array}\right), {\tau}_{a,a'} \sim 1-\text{Beta}(\alpha, \beta)/4
\end{equation}

I set the values corresponding to different loci to zero, as off-target probe hybridisation is unlikely to result in a signal.

\subsubsection*{Clone specific allelic variations}

The detection rate matrix, $\iota$, registers the mean clonal expression level. However, we can reasonably assume that different clones exhibit some level of diversity in gene expression. To model this deviation, I use a matrix $\gamma \in \mathbb{R}_{+}^{|s|\times|a|}$, with a LogNormal distributed prior such that $\mathbb{E}[\gamma]$ = 1.

\begin{equation} 
\gamma_{s,a} \sim \text{LogNormal}(\mu, \sigma)
\end{equation}

Regularising this prior is crucial to ensure that most of the probability density surrounds 1 by choosing a small $\sigma$. Since the matrix $\gamma$ has the same shape as the genotype matrix $\mathbf{G}$, failing to control this parameter could lead to non-identifiabilities.

\subsubsection*{Homogeneous and inhomogeneous background adjustments}

Raw BaSISS data are derived from biological images that include additional sources of pixel-level variability. The first source is a homogeneous additive shift $\beta \in \mathbb{R}_{+}^{|a|}$, which globally increases the expected value of a particular probe detection on the slide. This is consistent with a potential global background autofluoresence which differs in the wavelength, thus affecting each probe independently, as they are combinatorically encoded by fluorophores. In combination with the failed probe detection, this effect can lead to a global shift in the signal density. I model this effect as a Gamma distributed random variable.

\begin{equation} 
\beta_{a} \sim \text{Gamma}(\mu=0.5, \sigma=1) 
\end{equation}

The second source is an inhomogeneous background shift that adjusts for the variability in local base signal detection. Such factor should help the model explaining changes in signal composition that are not explainable by the linear combination of a known clonal genotypes. In practice, besides unknown clones, this factor tend to help with the technical noise, when some of the tissue areas are not properly registered, or in the necrotic areas, where nulcei count is high, but the \ac{BaSISS} signal is low.

Modelling the variance of this type without imposing harsh regularisation presents a challenge due to its flexibility. To represent the inhomogeneous background shift, I introduce additional pseudo-clones, denoted by $\psi$, with corresponding relative prevalence $\mathbf{M}$ and pseudo-genotype $\Gamma \in \mathbb{R}^{|p|\times|a|}_{+}$. The prevalence in this context mirrors that modelled for the actual clones. The pseudo-genotype is inferred from a Beta prior, which is manipulated to stretch the support to match the median copy number $k$ among the loci. Although I use $|\psi| = 1$, this value can be increased to allow for a more flexible background shift.

\begin{equation} 
    {\Gamma}_{p,a} \sim \text{Beta}(\alpha, \beta) \times k \\
\end{equation}

The Beta distirubtion parameters are chosen to shift the probability density towards zero, which should prevent the pseudo-genotype from being overestimated and overused by the model.

\subsubsection*{Sampling fluctuations}

I model BaSISS counts as being Negative Binomial distributed given an unobserved mean detection level $\mathbf{M}_{x,y,a}$ and over-dispersion parameter ${\alpha}_a$ which accounts for unexplained variance:

\begin{equation} 
\mathbf{D}_{x,y,a} \sim \text{NB}(\mu_{x,y,a}, \alpha_a)
\end{equation}

Over-dispersion $\alpha_a$ is sampled from Gamma distribution where distribution density is shifted towards larger value, which 

\begin{equation} 
{\alpha}_a \sim \mathrm{Gamma}(\mu, \sigma)
\end{equation} 


\subsection{Regularisation with auxiliary constraints}
\subsection{Multi-sample extensions}
\subsection{Inference}

\section{Validation}

\section{Model limitations and future directions}

\section{Discussion}
